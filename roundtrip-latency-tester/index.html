<script id=processor type="audio/worklet">
function mod(n, m) {
  return ((n % m) + m) % m;
}
class MeasureProcessor extends AudioWorkletProcessor {
  constructor(...args) {
    super(args);
    this.interval = 1 * globalThis.sampleRate;
    this.remaining = this.interval;
    this.start = 0;
    this.tapped = false;

    // Noise burst synthesis parameter
    this.sq_frames = 64;
    this.sq_remaining = 64;
    this.sq_period = 16;
    this.sq_amp = 0.8;
    // A ring buffer that always keep the last 1000ms of audio to be able to find
    // the beginning of the noise burst a peak has been found.
    this.ringbuf = new Float32Array(globalThis.sampleRate);
    this.write_idx = 0;
    var self = this;
    this.port.onmessage = function(e) {
      self.threshold = e.data.threshold;
    }
  }
  // record a single sample in the ring buffer
  record(sample) {
    this.ringbuf[this.write_idx] = sample;
    this.write_idx = mod(this.write_idx+1, this.ringbuf.length);
  }
  // get a sample from the ring buffer. idx is an offset in the past, 0 is the
  // sample most recently written to the ring buffer
  get_past_sample(idx) {
    var not_wrapped = this.write_idx - 1 - idx;
    var i = mod(this.write_idx-1-idx,this.ringbuf.length);
    return this.ringbuf[i];
  }
  process(inputs, outputs) {
    var input = inputs[0];
    if (!input.length) {
      return true;
    }
    var mono_input = input[0];
    var mono_output = outputs[0][0];
    for (var i = 0; i < mono_input.length; i++) {
      // This matches on a positive peak
      if (mono_input[i] > this.threshold && this.tapped) {
        // try to find the beginning of the pattern, because what's been found
        // is probably a peak, which is in the middle of the burst. Scan
        // back the last 10ms or so.
        var idx_first_zero_crossing = -1;
        var scan_back_idx = 0;
        while (scan_back_idx++ != this.ringbuf.length) {
          if (this.get_past_sample(scan_back_idx) < 0) {
            idx_first_zero_crossing = scan_back_idx;
            break;
          }
        }
        // we expect zero crossing around each 8 frames. Stop when that's not
        // the case anymore. This is not very good, this should be scanning
        // window + correlation maximisation.
        var sign = true;
        var current_period = 0;
        while (scan_back_idx++ != this.ringbuf.length) {
          var computed_period = (scan_back_idx - idx_first_zero_crossing) / this.sq_period;
          if (sign != Math.sign(this.get_past_sample(scan_back_idx))) {
            // zero crossing, fuzz match
            if (Math.abs(current_period - computed_period) > 2) {
              // too far away from the generated burst, break and consider this
              // the beginning of the burst.
              break;
            }
          }
        }
        // send back frames from the past to the main thread to display in debug
        // mode
        var frames_delay = (globalThis.currentFrame + i - scan_back_idx) - this.start;
        if (frames_delay > 0) {
          var debugarray = new Float32Array(frames_delay * 2);
          var rdIdx = 0;
          for (var j = 0; j < debugarray.length; j++) {
            debugarray[debugarray.length - j] = this.get_past_sample(j);
          }
          var latency_s = frames_delay / globalThis.sampleRate;
          this.port.postMessage({latency: latency_s,
                                 array: debugarray,
                                 offset: scan_back_idx,
                                 delay_frames: frames_delay});
        }
        this.tapped = false;
      }
      if (this.remaining == 0) {
        if (this.sq_remaining == this.sq_frames) {
          this.tapped = true;
          this.start = globalThis.currentFrame + i;
          mono_input[i] = -1.0;
        }
        mono_output[i] = (this.sq_remaining % this.sq_period) >
        this.sq_period/2 ? this.sq_amp : -this.sq_amp;
        this.sq_remaining--;
        if (this.sq_remaining == 0) {
          this.sq_remaining = this.sq_frames;
          this.remaining = this.interval;
        }
      } else {
        this.remaining--;
      }
      this.record(mono_input[i] + mono_output[i]);
      mono_output[i] += mono_input[i];
    }
    return true;
  }
}

registerProcessor('measure-processor', MeasureProcessor);
</script>
<meta charset=utf-8>
<style>
body {
  font-size: 24px;
}
canvas {
  border: 1px solid black;
  display: inline;
}
input {
  padding: 0px;
  margin-top: 0px;
  margin-bottom: 384px;
  height: 384px;
  width: 15px;
  -webkit-appearance: slider-vertical;
  display: inline;
}
html {
  font-family: sans-serif;
}
#wrapperr {
  display: flex;
  flex-direction: column;
  justify-content: center;
}
.wrapper {
  justify-content: center;
  flex-direction: row;
  display: flex;
  max-width: 95%;
}
#explanations {
  display: block;
  margin-left: 2em;
}
#latency, #computed {
  font-weight: 700;
}
</style>
<div id=wrapperr>
  <div class=wrapper>
    <div id=explanations>
      <h1>Audio Roundtrip latency measurements</h1>
      <p>
      In a quiet environment, either:
      </p>
      <ul>
        <li>using the speakers of a computer, and the microphone of the computer
        <li>using headphones, and their microphone, or regular headphones and
        the microphone of the computer
      </ul>
      <p>
      Lower the volume of the output device, and in the case of headphones
      bring it closer to the mic (if at all possible).
      </p>
      <p>
      Click the button, and accept to share the microphone (it's not going
      anywhere, the code is simple just look at it).
      </p>
      <p>
      Gradually and slowly increase the output volume of the computer until a
      noise burst is hearable, and seen on the page, but low enough to not
      cause feedback.
      </p>
      <p>
      Adjust the red line with the slider, so that it falls just below the
      peaks of the waveform. When this is the case, note and report the two
      numbers in bold, along with the setup and contact info <a href="https://docs.google.com/spreadsheets/d/17rU23jfUFf3J6_YVsufon_eDP7OE_Lx-kNbCgqLeLxI/edit?usp=sharing">here</a>.
      </p>
      <p>
      For any question contact <code>padenot@mozilla.com</code>
      </p>
      <button id=start>Start measure</button>
      <button id=stop>Stop measure</button>
      <p><code>AudioContext sample-rate: </code>: <span id=sample-rate>...</span></p>
      <p>Measured rountrip: <span id=latency>...</span></p>
      <p>Computed output latency: <span id=computed>...</span></p>
    </div>
  </div>
  <div class=wrapper>
    <input type=range value=0.2 min=0.0 max=1.0 step=0.01 vertical orient="vertical">
    <canvas width=1024 height=768></canvas>
    <canvas id=debug width=1024 height=512></canvas>
    <div>
    </div>
  </div>
</div>

<script>
function main() {
  if (!window.AudioWorkletNode) {
    alert("This experiment requires AudioWorklet, please try this on Firefox beta 76 and more recent");
    return;
  }
  $ = document.querySelectorAll.bind(document);
  $("button#start")[0].onclick = start;
  $("button#stop")[0].onclick = stop;
  $("button#stop")[0].disabled = true;
  const text = $('#processor')[0].innerText;
  const blob = new Blob([text], {type: "application/javascript"});
  const url = URL.createObjectURL(blob);
  var ac = new AudioContext();
  $("#sample-rate")[0].innerText = ac.sampleRate;
  var canvas = $("canvas")[0];
  var canvas_debug = $("canvas")[1];
  if (window.innerWidth < 1024) {
    canvas.width = window.innerWidth * 0.95;
    canvas.height = canvas.width * 0.75;
  }
  var w = canvas.width;
  var h = canvas.height;
  var ctx = canvas.getContext("2d");
  var analyser = new AnalyserNode(ac);
  analyser.fftSize = 2048;
  var buf = new Float32Array(analyser.frequencyBinCount);
  var cvs_step = w / buf.length;
  var initialSetup = false;

  var inputProcessing = document.location.search.indexOf("input-processing") != -1;
  var debugCanvas = document.location.search.indexOf("debug") != -1;

  console.log("Input processing: ", inputProcessing);
  console.log("Debug canvas", debugCanvas);

  if (!debugCanvas) {
    canvas_debug.remove();
  }

  threshold = $("input[type=range]")[0].value;

  function draw() {
    ctx.fillStyle = "rgba(255, 255, 255, 0.05)";
    ctx.fillRect(0, 0, w, h);
    ctx.fillStyle = "#333";
    analyser.getFloatTimeDomainData(buf);
    var acc = 0;
    var t = true;
    [parseFloat(threshold), -0.75,-0.5,-0.25, 0, 0.25, 0.5, 0.75].forEach(function(v) {
      if (t) {
        ctx.strokeStyle = "#a00";
        t = false;
      } else {
        ctx.strokeStyle = "#aaa";
      }
      ctx.fillText(v, 3, h/2 - (v * h/2) + 3);
      ctx.beginPath();
      ctx.moveTo(30, h/2 - (v * h/2));
      ctx.lineTo(w-30, h/2 - (v * h/2));
      ctx.stroke();
      ctx.closePath();
    });
    for (var i = 0; i < buf.length; i++) {
      ctx.fillRect(acc, h/2, cvs_step, buf[i] * h / 2);
      acc+=cvs_step;
    }
    requestAnimationFrame(draw);
  }

  if (stop) {
    requestAnimationFrame(draw);
  }

  function stop() {
    ac.suspend();
    $("button#stop")[0].disabled = true;
    $("button#start")[0].disabled = false;
    stop = true;
  }

  async function start() {
    $("button#stop")[0].disabled = false;
    $("button#start")[0].disabled = true;
    stop = false;
    draw();
    await ac.resume();
    try {
      await ac.resume();
      if (!initialSetup) {
        await ac.audioWorklet.addModule(url);
        var constraints = inputProcessing ? { audio : {
          // disabling this because we're intentionally trying to echo 
          echoCancellation: false,
          noiseSupression: true,
          autoGainControl: true
        }} : { audio: {
          echoCancellation: false,
          noiseSupression: false,
          autoGainControl: false
        }};
        let stream = await navigator.mediaDevices.getUserMedia(constraints);

        var mic_source = ac.createMediaStreamSource(stream);
        var worklet_node = new AudioWorkletNode(ac, 'measure-processor', {outputChannelCount: [1]});
        worklet_node.channelCount = 1;
        mic_source.connect(analyser);
        mic_source.connect(worklet_node).connect(ac.destination);

        worklet_node.port.postMessage({threshold: $("input")[0].value });


        worklet_node.port.onmessage = function(e) {
          if (debugCanvas) {
            var c2 = canvas_debug.getContext("2d");
            var w2 = canvas_debug.width;
            var h2 = canvas_debug.height;
            var between_peaks = e.data.array;
            var len = e.data.array.length;
            var wstep = w2 / len;

            c2.clearRect(0, 0, w2, h2);

            c2.fillStyle = "#000";
            var x = 0;
            for (var i = 0; i < len; i++) {
              c2.fillRect(x, h2/2, wstep, between_peaks[i] * h2/2);
              x += wstep;
            }
            c2.fillStyle = "#f00";
            c2.fillRect(w2 - wstep * e.data.offset, 0, wstep, h);

            c2.fillRect(w2 - wstep * e.data.delay_frames - wstep * e.data.offset, h2 *
                0.75, w2 - wstep * e.data.delay_frames, 4);
          }

          $("#latency")[0].innerText = (e.data.latency * 1000) + "ms"
          $("#computed")[0].innerText = (ac.outputLatency * 1000)+ "ms"
        }

        $("input[type=range]")[0].oninput = (e) => {
          threshold = e.target.value;
          worklet_node.port.postMessage({threshold: e.target.value});
        };
        initialSetup = true;
      }
    } catch (e) {
    }
  }
}
main();
</script>
